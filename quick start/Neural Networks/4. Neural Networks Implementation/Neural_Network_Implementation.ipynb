{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "### By **[NimbleBox](https://www.nimblebox.ai)**\n",
    "\n",
    "\n",
    "[<img src=\"./assets/nbx.jpeg\" alt=\"NimbleBox.ai logo\" width=\"600\"/>](https://www.nimblebox.ai)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What will we do\n",
    " \n",
    "In this notebook we are going to implement a neural network from scratch in python and this time we are going to build a different neural network then the one that we used to explain neural networks. This time we are going to solve a classification problem. So there will be some minor changes to the network to use it for Classification. Let's look at them.\n",
    " \n",
    "## Softmax \n",
    " \n",
    "A softmax function takes a vector of input and gives out a vector with probabilities that obviously add up to one because they are probabilities and in our case a softmax unit at the end will tell us the probability of the class that the neural network thinks it is.\n",
    " \n",
    "Softmax is calculated by taking the elementwise exponent of the vector and then dividing elementwise by the sum of the vector after taking the exponent.\n",
    " \n",
    "<img src=\"./assets/softmax.png\" width=150>\n",
    " \n",
    " \n",
    "so let's take an example in our implementation we will be using the iris dataset which has 3 flowers['setosa', 'versicolor', 'virginica'] in the same order so for one example the true output looks like this.\n",
    " \n",
    "$$ Y = [0, 1, 0] $$\n",
    " \n",
    "This means that the second node in our neural network denotes a particular flower which is versicolor and we will be expecting a softmax output something like this.\n",
    " \n",
    "$$ Y_hat = [0.3, 0.95, 0.2] $$\n",
    " \n",
    "We pick the largest number in the vector to decide which class the neural network thinks this example denotes to.\n",
    " \n",
    "## Loss Function\n",
    " \n",
    "We will also use a new loss function named **Cross Entropy loss** to measure the classification loss between multiple classes.\n",
    " \n",
    "the formula for the loss is this where $ y_o $ is Y or the labels and $ P_o $ is the predicted label which you will also see in the below implementation. \n",
    " \n",
    "<img src=\"./assets/cross_entropy.png\" width=150>\n",
    " \n",
    " \n",
    "we will change the input units or the inputs($ X $) to 4 and the number of hidden units to 5 and the output is going to be 3 units as there are 3 flowers Now let's see the architecture of the neural network.\n",
    " \n",
    "<img src=\"./assets/nn.png\" width=700>\n",
    " \n",
    "The Forward, Backward and Gradient steps are going to remain the same, the only difference is that now instead of a scalar they are going to be matrices. Let's have a look over these steps again and I will be mentioning the matrix dimensions in front of all the variables this time.\n",
    " \n",
    "### Forward Propagation step\n",
    " \n",
    "As we know that we will be using a sigmoid function instead of $ g() $ and a softmax function at the end I will also be replacing that.\n",
    " \n",
    "1. $ Z_1 = W_1[5,4] * X[4,150] + b_1[5, 1]$\n",
    "2. $ A_1 = sigmoid(Z)[5, 150] $\n",
    "3. $ Z_2 = softmax(W_2[3, 5] * A_1[5,150] + b_2[3 ,1])$\n",
    "4. $ Y\\_hat = Z_2[3, 150] $\n",
    " \n",
    "### Backward Propagation step\n",
    " \n",
    "For backward Propagation we only used $ W1 $ as our example and the change their is that instead of calculating the derivative for a scalar we will be calculating the derivative for the whole vector or matrix but element wise. Let's take an example and suppose we had a vector A like the one below. \n",
    " \n",
    "$$ A = [2, 4, 5] $$\n",
    " \n",
    "So if say do an element wise square on $ A $, The result will be.\n",
    " \n",
    "$$ A^2 = [4, 16, 25] $$\n",
    " \n",
    "And as the loss function is changed the derivative of $ ∂E/∂Y\\_hat $ is also changed.\n",
    " \n",
    "1. $ ∂E/∂Y\\_hat = Y_hat - Y $\n",
    "2. $ ∂Y\\_hat/∂A = W_2 $\n",
    "3. $ ∂A/∂Z = A*(1 - A) $\n",
    "4. $ ∂Z/∂W_1 = X $\n",
    " \n",
    "finally the whole derivative for $ ∂E/∂W_1 $ will be.\n",
    " \n",
    "$$ ∂E/∂W_1 = (Y_hat - Y) * W_2 * A*(1 - A) * X $$\n",
    " \n",
    "### Gradient descent\n",
    " \n",
    "$$ W_1[5, 4] = W_1[5, 4] - α[1, 1] * ∂E/∂W_1[5, 4] $$ \n",
    "$$ b_1[5, 1] = b_1[5, 1] - α[1, 1] * ∂E/∂b_1[5, 1] $$\n",
    "$$ W_2[3, 5] = W_2[3, 5] - α[1, 1] * ∂E/∂W_2[3, 5] $$\n",
    "$$ b_2[3, 1] = b_2[3, 1] - α[1, 1] * ∂E/∂b_2[3, 1] $$\n",
    " \n",
    "Where $ α $ is going to be a scalar which we will broadcast to match the shape of our matrix to which it subtracts with. \n",
    " \n",
    "## Implementation\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  0    loss :  714.248039960841\n",
      "iteration :  5    loss :  574.7556644840092\n",
      "iteration :  10    loss :  526.8132917923971\n",
      "iteration :  15    loss :  507.89264044261654\n",
      "iteration :  20    loss :  500.36320086007197\n",
      "iteration :  25    loss :  497.3204452129758\n",
      "iteration :  30    loss :  496.02228412648174\n",
      "iteration :  35    loss :  495.41901638734294\n",
      "iteration :  40    loss :  495.10576238347045\n",
      "iteration :  45    loss :  494.9210980924519\n",
      "iteration :  50    loss :  494.7984504907715\n",
      "iteration :  55    loss :  494.70951852235754\n",
      "iteration :  60    loss :  494.6416982207869\n",
      "iteration :  65    loss :  494.58880638800525\n",
      "iteration :  70    loss :  494.54728260335895\n",
      "iteration :  75    loss :  494.5146887324421\n",
      "iteration :  80    loss :  494.4891492113893\n",
      "iteration :  85    loss :  494.4691519159778\n",
      "iteration :  90    loss :  494.4534718772057\n",
      "iteration :  95    loss :  494.44112900863485\n",
      "iteration :  100    loss :  494.43135205100435\n",
      "iteration :  105    loss :  494.4235432844798\n",
      "iteration :  110    loss :  494.4172450459722\n",
      "iteration :  115    loss :  494.4121098636282\n",
      "iteration :  120    loss :  494.40787523567707\n",
      "iteration :  125    loss :  494.4043432141038\n",
      "iteration :  130    loss :  494.4013643899927\n",
      "iteration :  135    loss :  494.3988256164558\n",
      "iteration :  140    loss :  494.3966407492782\n",
      "iteration :  145    loss :  494.3947437411658\n",
      "iteration :  150    loss :  494.3930835279069\n",
      "iteration :  155    loss :  494.3916202562029\n",
      "iteration :  160    loss :  494.3903225047782\n",
      "iteration :  165    loss :  494.38916523558817\n",
      "iteration :  170    loss :  494.3881282795186\n",
      "iteration :  175    loss :  494.38719521277363\n",
      "iteration :  180    loss :  494.38635251895045\n",
      "iteration :  185    loss :  494.3855889604142\n",
      "iteration :  190    loss :  494.3848951034952\n",
      "iteration :  195    loss :  494.38426295719233\n",
      "iteration :  200    loss :  494.3836856960412\n",
      "iteration :  205    loss :  494.3831574457305\n",
      "iteration :  210    loss :  494.38267311578176\n",
      "iteration :  215    loss :  494.3822282677572\n",
      "iteration :  220    loss :  494.3818190104731\n",
      "iteration :  225    loss :  494.3814419158899\n",
      "iteration :  230    loss :  494.3810939509581\n",
      "iteration :  235    loss :  494.38077242187416\n",
      "iteration :  240    loss :  494.3804749280698\n",
      "iteration :  245    loss :  494.3801993239045\n",
      "iteration :  250    loss :  494.37994368650396\n",
      "iteration :  255    loss :  494.37970628854896\n",
      "iteration :  260    loss :  494.3794855750882\n",
      "iteration :  265    loss :  494.37928014364684\n",
      "iteration :  270    loss :  494.37908872706197\n",
      "iteration :  275    loss :  494.37891017859084\n",
      "iteration :  280    loss :  494.37874345892635\n",
      "iteration :  285    loss :  494.3785876248303\n",
      "iteration :  290    loss :  494.3784418191407\n",
      "iteration :  295    loss :  494.37830526196063\n",
      "iteration :  300    loss :  494.3781772428629\n",
      "iteration :  305    loss :  494.3780571139769\n",
      "iteration :  310    loss :  494.3779442838422\n",
      "iteration :  315    loss :  494.37783821193227\n",
      "iteration :  320    loss :  494.3777384037662\n",
      "iteration :  325    loss :  494.3776444065385\n",
      "iteration :  330    loss :  494.37755580520394\n",
      "iteration :  335    loss :  494.37747221896757\n",
      "iteration :  340    loss :  494.37739329813166\n",
      "iteration :  345    loss :  494.3773187212612\n",
      "iteration :  350    loss :  494.37724819263184\n",
      "iteration :  355    loss :  494.37718143993095\n",
      "iteration :  360    loss :  494.3771182121821\n",
      "iteration :  365    loss :  494.3770582778728\n",
      "iteration :  370    loss :  494.37700142326116\n",
      "iteration :  375    loss :  494.3769474508437\n",
      "iteration :  380    loss :  494.3768961779687\n",
      "iteration :  385    loss :  494.3768474355787\n",
      "iteration :  390    loss :  494.3768010670707\n",
      "iteration :  395    loss :  494.3767569272601\n",
      "iteration :  400    loss :  494.37671488144076\n",
      "iteration :  405    loss :  494.3766748045291\n",
      "iteration :  410    loss :  494.3766365802859\n",
      "iteration :  415    loss :  494.37660010060705\n",
      "iteration :  420    loss :  494.3765652648768\n",
      "iteration :  425    loss :  494.37653197937834\n",
      "iteration :  430    loss :  494.3765001567559\n",
      "iteration :  435    loss :  494.3764697155217\n",
      "iteration :  440    loss :  494.3764405796073\n",
      "iteration :  445    loss :  494.37641267795095\n",
      "iteration :  450    loss :  494.37638594412164\n",
      "iteration :  455    loss :  494.37636031597253\n",
      "iteration :  460    loss :  494.3763357353253\n",
      "iteration :  465    loss :  494.3763121476785\n",
      "iteration :  470    loss :  494.3762895019411\n",
      "iteration :  475    loss :  494.37626775018623\n",
      "iteration :  480    loss :  494.37624684742656\n",
      "iteration :  485    loss :  494.37622675140545\n",
      "iteration :  490    loss :  494.3762074224062\n",
      "iteration :  495    loss :  494.37618882307555\n",
      "iteration :  500    loss :  494.3761709182605\n",
      "iteration :  505    loss :  494.3761536748592\n",
      "iteration :  510    loss :  494.3761370616812\n",
      "iteration :  515    loss :  494.37612104932\n",
      "iteration :  520    loss :  494.3761056100343\n",
      "iteration :  525    loss :  494.37609071763836\n",
      "iteration :  530    loss :  494.37607634740056\n",
      "iteration :  535    loss :  494.37606247594914\n",
      "iteration :  540    loss :  494.3760490811852\n",
      "iteration :  545    loss :  494.3760361422019\n",
      "iteration :  550    loss :  494.3760236392087\n",
      "iteration :  555    loss :  494.3760115534625\n",
      "iteration :  560    loss :  494.37599986720227\n",
      "iteration :  565    loss :  494.3759885635889\n",
      "iteration :  570    loss :  494.3759776266491\n",
      "iteration :  575    loss :  494.3759670412231\n",
      "iteration :  580    loss :  494.3759567929162\n",
      "iteration :  585    loss :  494.3759468680533\n",
      "iteration :  590    loss :  494.3759372536364\n",
      "iteration :  595    loss :  494.3759279373057\n",
      "iteration :  600    loss :  494.3759189073022\n",
      "iteration :  605    loss :  494.37591015243356\n",
      "iteration :  610    loss :  494.37590166204166\n",
      "iteration :  615    loss :  494.37589342597323\n",
      "iteration :  620    loss :  494.3758854345506\n",
      "iteration :  625    loss :  494.3758776785463\n",
      "iteration :  630    loss :  494.3758701491579\n",
      "iteration :  635    loss :  494.3758628379848\n",
      "iteration :  640    loss :  494.3758557370071\n",
      "iteration :  645    loss :  494.3758488385644\n",
      "iteration :  650    loss :  494.3758421353377\n",
      "iteration :  655    loss :  494.3758356203301\n",
      "iteration :  660    loss :  494.37582928685174\n",
      "iteration :  665    loss :  494.37582312850225\n",
      "iteration :  670    loss :  494.375817139157\n",
      "iteration :  675    loss :  494.37581131295224\n",
      "iteration :  680    loss :  494.37580564427276\n",
      "iteration :  685    loss :  494.3758001277388\n",
      "iteration :  690    loss :  494.37579475819444\n",
      "iteration :  695    loss :  494.3757895306969\n",
      "iteration :  700    loss :  494.37578444050575\n",
      "iteration :  705    loss :  494.37577948307325\n",
      "iteration :  710    loss :  494.3757746540351\n",
      "iteration :  715    loss :  494.37576994920175\n",
      "iteration :  720    loss :  494.3757653645496\n",
      "iteration :  725    loss :  494.37576089621393\n",
      "iteration :  730    loss :  494.37575654048106\n",
      "iteration :  735    loss :  494.37575229378155\n",
      "iteration :  740    loss :  494.3757481526833\n",
      "iteration :  745    loss :  494.3757441138855\n",
      "iteration :  750    loss :  494.3757401742125\n",
      "iteration :  755    loss :  494.3757363306087\n",
      "iteration :  760    loss :  494.3757325801324\n",
      "iteration :  765    loss :  494.3757289199515\n",
      "iteration :  770    loss :  494.3757253473382\n",
      "iteration :  775    loss :  494.37572185966434\n",
      "iteration :  780    loss :  494.375718454398\n",
      "iteration :  785    loss :  494.3757151290979\n",
      "iteration :  790    loss :  494.3757118814111\n",
      "iteration :  795    loss :  494.37570870906814\n",
      "iteration :  800    loss :  494.37570560987984\n",
      "iteration :  805    loss :  494.375702581734\n",
      "iteration :  810    loss :  494.37569962259215\n",
      "iteration :  815    loss :  494.37569673048677\n",
      "iteration :  820    loss :  494.37569390351786\n",
      "iteration :  825    loss :  494.3756911398508\n",
      "iteration :  830    loss :  494.37568843771305\n",
      "iteration :  835    loss :  494.37568579539226\n",
      "iteration :  840    loss :  494.3756832112336\n",
      "iteration :  845    loss :  494.37568068363737\n",
      "iteration :  850    loss :  494.37567821105705\n",
      "iteration :  855    loss :  494.375675791997\n",
      "iteration :  860    loss :  494.375673425011\n",
      "iteration :  865    loss :  494.3756711086993\n",
      "iteration :  870    loss :  494.37566884170803\n",
      "iteration :  875    loss :  494.37566662272684\n",
      "iteration :  880    loss :  494.3756644504869\n",
      "iteration :  885    loss :  494.3756623237605\n",
      "iteration :  890    loss :  494.375660241358\n",
      "iteration :  895    loss :  494.3756582021277\n",
      "iteration :  900    loss :  494.37565620495354\n",
      "iteration :  905    loss :  494.3756542487546\n",
      "iteration :  910    loss :  494.37565233248307\n",
      "iteration :  915    loss :  494.37565045512326\n",
      "iteration :  920    loss :  494.37564861569115\n",
      "iteration :  925    loss :  494.3756468132318\n",
      "iteration :  930    loss :  494.3756450468198\n",
      "iteration :  935    loss :  494.37564331555734\n",
      "iteration :  940    loss :  494.37564161857335\n",
      "iteration :  945    loss :  494.375639955023\n",
      "iteration :  950    loss :  494.37563832408625\n",
      "iteration :  955    loss :  494.37563672496725\n",
      "iteration :  960    loss :  494.3756351568934\n",
      "iteration :  965    loss :  494.37563361911475\n",
      "iteration :  970    loss :  494.3756321109031\n",
      "iteration :  975    loss :  494.3756306315511\n",
      "iteration :  980    loss :  494.3756291803718\n",
      "iteration :  985    loss :  494.3756277566977\n",
      "iteration :  990    loss :  494.3756263598807\n",
      "iteration :  995    loss :  494.3756249892906\n",
      "iteration :  1000    loss :  494.37562364431494\n",
      "iteration :  1005    loss :  494.3756223243587\n",
      "iteration :  1010    loss :  494.37562102884317\n",
      "iteration :  1015    loss :  494.3756197572059\n",
      "iteration :  1020    loss :  494.37561850889966\n",
      "iteration :  1025    loss :  494.3756172833924\n",
      "iteration :  1030    loss :  494.37561608016665\n",
      "iteration :  1035    loss :  494.37561489871905\n",
      "iteration :  1040    loss :  494.37561373855965\n",
      "iteration :  1045    loss :  494.3756125992119\n",
      "iteration :  1050    loss :  494.37561148021194\n",
      "iteration :  1055    loss :  494.37561038110834\n",
      "iteration :  1060    loss :  494.3756093014617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration :  1065    loss :  494.3756082408439\n",
      "iteration :  1070    loss :  494.37560719883857\n",
      "iteration :  1075    loss :  494.37560617504\n",
      "iteration :  1080    loss :  494.3756051690532\n",
      "iteration :  1085    loss :  494.3756041804931\n",
      "iteration :  1090    loss :  494.375603208985\n",
      "iteration :  1095    loss :  494.37560225416365\n",
      "iteration :  1100    loss :  494.3756013156731\n",
      "iteration :  1105    loss :  494.37560039316656\n",
      "iteration :  1110    loss :  494.3755994863062\n",
      "iteration :  1115    loss :  494.3755985947624\n",
      "iteration :  1120    loss :  494.3755977182141\n",
      "iteration :  1125    loss :  494.3755968563483\n",
      "iteration :  1130    loss :  494.37559600885953\n",
      "iteration :  1135    loss :  494.37559517545026\n",
      "iteration :  1140    loss :  494.37559435583023\n",
      "iteration :  1145    loss :  494.37559354971614\n",
      "iteration :  1150    loss :  494.3755927568319\n",
      "iteration :  1155    loss :  494.37559197690797\n",
      "iteration :  1160    loss :  494.3755912096815\n",
      "iteration :  1165    loss :  494.375590454896\n",
      "iteration :  1170    loss :  494.3755897123011\n",
      "iteration :  1175    loss :  494.37558898165264\n",
      "iteration :  1180    loss :  494.37558826271203\n",
      "iteration :  1185    loss :  494.3755875552465\n",
      "iteration :  1190    loss :  494.375586859029\n",
      "iteration :  1195    loss :  494.3755861738376\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, preprocessing\n",
    "\n",
    "def sigmoid(X):\n",
    "  return 1 / (1 + np.exp(-X))\n",
    "\n",
    "def sigmoid_der(x):\n",
    "  return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def softmax(X):\n",
    "  e = np.exp(X - X.max())\n",
    "  return (e / e.sum(axis=1, keepdims=True))\n",
    "\n",
    "def loss(Y, Y_hat):\n",
    "  loss = Y * np.log(Y_hat)\n",
    "  return -np.sum(loss)\n",
    "\n",
    "def neural_network_train(X, Y, num_iteration=1200):\n",
    "  # Random initializing the weights and bias\n",
    "\n",
    "  W_1 = np.random.randn(5, 4)\n",
    "  b_1 = np.random.randn(1, 5)\n",
    "  \n",
    "  W_2 = np.random.randn(3, 5)\n",
    "  b_2 = np.random.randn(1, 3)\n",
    "\n",
    "  # Defining the learning rate\n",
    "\n",
    "  lr = 1e-3\n",
    "\n",
    "  for iteration in range(num_iteration):\n",
    "      # Forward Propagation\n",
    "\n",
    "    Z_1 = np.dot(X, W_1.T) + b_1\n",
    "    A_1 = sigmoid(Z_1)\n",
    "    Z_2 = np.dot(A_1, W_2.T) + b_2\n",
    "    Y_hat = softmax(Z_2)\n",
    "\n",
    "    # Backward Propagation\n",
    "\n",
    "    dE_dY_hat = Y_hat - Y\n",
    "    dY_hat_dW_2 = A_1\n",
    "\n",
    "    dE_dW_2 = np.dot(dY_hat_dW_2.T, dE_dY_hat)\n",
    "\n",
    "    dE_db_2 = dE_dY_hat\n",
    "\n",
    "    dZ_2_dA_1 = W_2\n",
    "    dE_dA_1 = np.dot(dE_dY_hat, dZ_2_dA_1)\n",
    "    dA_1_dZ_1 = sigmoid_der(Z_1)\n",
    "    dZ_1_dW_1 = X\n",
    "    dE_dW_1 = np.dot(dZ_1_dW_1.T, dA_1_dZ_1 * dE_dA_1)\n",
    "\n",
    "    dE_db_1 = dE_dA_1 * dA_1_dZ_1\n",
    "\n",
    "    # Gradient Descent\n",
    "\n",
    "    W_2 = W_2 - lr * dE_dW_2.T\n",
    "    b_2 = b_2 - lr * dE_db_2.sum(axis=0)\n",
    "\n",
    "    W_1 = W_1 - lr * dE_dW_1.T\n",
    "    b_1 = b_1 - lr * dE_db_1.sum(axis=0)\n",
    "\n",
    "\n",
    "    if iteration % 5 == 0:\n",
    "      print(\"iteration : \", iteration, \"   loss : \", loss(Y, Y_hat))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  X, Y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "  # Preprocessing the data to have a mean 0 and variance 1\n",
    "  X = preprocessing.scale(X)\n",
    "  # Y has the shape (150, ) rather than (150, 1)\n",
    "  Y = Y.reshape(150,1)\n",
    "  neural_network_train(X, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to do next\n",
    " \n",
    "As you can see that we haven't chosen the best parameters. There are a lot of things that you can tune. let's mention some of them.\n",
    " \n",
    "- Learning rate.\n",
    "- Number of hidden units.\n",
    "- Number of hidden layers.\n",
    "- weight initialization method.\n",
    "- activation function used: You can use ReLU or any other activation function instead of sigmoid.\n",
    "- number of iterations.\n",
    "- Type of classification loss.\n",
    "- Different optimization algorithm such as mini-batch gradient descent, Adam or RMSprop."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
