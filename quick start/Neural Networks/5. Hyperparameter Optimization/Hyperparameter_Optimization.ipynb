{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "### By **[NimbleBox](https://www.nimblebox.ai)**\n",
    "\n",
    "[<img src=\"./assets/nbx.jpeg\" alt=\"NimbleBox.ai logo\" width=\"600\"/>](https://www.nimblebox.ai)\n",
    "\n",
    "Parts of this notebook are from [Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization](https://www.coursera.org/learn/deep-neural-network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What will we learn \n",
    "\n",
    "In this Notebook we are going to learn the difference about parameters and hyperparameters, which hyperparameters has a bigger impact on improving the performance and accuracy then we are going to talk about some approaches to find hyperparameters and then we are going to talk about two approaches on how to tune your model and find the best hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters VS Hyperparameters\n",
    " \n",
    "Let's talk about their differences, So we have talked about weight and biases and these are some of the things that your model uses to predict the output given a certain input, These are called parameters whereas hyperparameters are the ones that you use to train your model and in turn modify parameters some examples of hyperparameters are learning rate, type of optimization algorithm, number of iterations in the optimization algorithm ,activation function, number of layers in a neural network about which we are going to study in the coming weeks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which Hyperparameters\n",
    " \n",
    "There are so many hyperparameters to tune and which should you tune first, So there is no hard rule that says that you have to tune so and so parameters first but as you practice machine learning you get an idea that certain hyperparameters have more impact on accuracy or the model performance than others and it's better to focus your efforts on that, but just to get you started we will list down some of them in descending order of their priority that you can try.\n",
    " \n",
    "- Learning rate.\n",
    "- Parameters of your optimization algorithm.\n",
    "- Mini batch size.\n",
    "- Hidden number of units in neural networks.\n",
    "- Hidden layers in a neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching \n",
    " \n",
    "So now you know what are hyperparameters and which ones are better to tune first or give more importance to, With that let's talk about how you are going to search them, so for this we are going to assume that for now we are only trying to search for two hyperparameters which can be anything from the ones mentioned above or something else too.\n",
    " \n",
    "### Grid Search \n",
    " \n",
    "So as the name says Grid search is a type of search in which you define the scale or the end points of the hyperparameters that you want to search and then you train and evaluate your model on every value on that grid to find a better pair of hyperparameters as there were only two in our case. Let's look at a picture to get a fair idea. \n",
    " \n",
    "<img src=\"./assets/grid_search.png\" width=500>\n",
    " \n",
    "### Random Grid Search\n",
    " \n",
    "Random grid search is similar to grid search except we now sample hyperparameters points to try at random. But why would we want to do such a thing because when we have grid search with which we explore every point and can find the best ones. Grid search even with just two parameters and a small scale to search is still very computationally expensive and in the era of deep learning where we don't just have two parameters but a lot of them on a larger scale it's not feasible with respect to computing power and the time that it will take to try every point out there. That's where random grid search comes in and practically it's very effective in finding a good pair of hyperparameters and mostly far more quickly and with less resources than grid search.\n",
    " \n",
    "<img src=\"./assets/random_grid_search.png\" width=500>\n",
    " \n",
    "As you can see in the image that the points are randomly sampled.\n",
    "\n",
    "So you can use grid search when the number of hyperparameters to tune is less and you should prefer random grid search when they are just too much.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching the best\n",
    " \n",
    "You can find good hyperparameters with grid search and random grid search but to get the best hyperparameters you do just one more step, When you see that you are getting good results with your model in a certain are of the grid you zoom in, what we mean by that is you just ignore all the other parts of the grid and sample more finer points in that area which can also be said as searching from a coarse to fine scale and you can repeat this process or you can again try a searching method and try a different location this time.\n",
    " \n",
    "<img src=\"./assets/coarse_to_fine.png\" width=700>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching scale\n",
    " \n",
    "So let's say you are trying to search for the best number of iterations for you optimization algorithm that you can use to achieve a good model and you think the scale for that will be somewhere around 20 to 100 so one the number line as shown below.\n",
    " \n",
    "<img src=\"./assets/optim_numline.png\" width=700>\n",
    " \n",
    "And you start to randomly sample a point uniformly for your hyperparameters and that is fine in such a case because you can explore every range starting from say 20 to 30, 30 to 40 and so on.\n",
    " \n",
    "But now let's take the case of Learning rate and they are generally in the range from 0.0001 to 1 but now if you start to randomly sample on such a number line most of the point that you are going to sample are going to be from the range of 0.1 to 1 and you will not be exploring most of the points in the range from 0.0001 to 0.1\n",
    " \n",
    "<img src=\"./assets/lr_numline.png\" width=700>\n",
    " \n",
    "To solve such a problem we sample points on a logarithmic number line where 0.0001 can be written as $ 10^{-4} $ and 1 as $ 10^0 $ this way you can sample and explore points more equally. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Approaches \n",
    " \n",
    "Now we know some of the searching techniques but how do we evaluate the parameters that we choose are for our model. People in the field of machine learning tend to use two types of techniques that we will discuss below.\n",
    " \n",
    "The first one is to train multiple models with different hyperparameters and see which one works best or achieves a good accuracy, you can do such a thing if your model is small or you have huge computing power. \n",
    " \n",
    "Another thing the people do is that they babysit the the model looking at it's training curve every day or after some hours and tune the hyperparameters manually be looking at the loss or accuracy or other metrics that they are tracking if the model get's bad result with some hyperparameters they go back to previous model and try another pair of hyperparameters and continue to train the model like this, This Approach is used if your model is really huge or if you have less computing power. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
