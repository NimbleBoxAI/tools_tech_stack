{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "### By **[NimbleBox](https://www.nimblebox.ai)**\n",
    "\n",
    "[<img src=\"./assets/nbx.jpeg\" alt=\"NimbleBox.ai logo\" width=\"600\"/>](https://www.nimblebox.ai)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What will we learn \n",
    " \n",
    "This week we are going to learn about what Convolutional Neural Networks(CNN) are and why we use them to process image data rather than just plain neural networks and after that we will learn about recurrent Neural Networks to process time-series data such as speech recognition or named entity recognition, named entity recognition just means to identify which word is a name in a sentence. We will use tensorflow to implement both types of network and will learn about transfer learning in computer vision applications through CNNs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Convolution in Convolutional Neural Networks \n",
    " \n",
    "So what actually is a Convolution in a CNN, Let's take two matrices **A** and **B** to understand what a Convolutional operation is.\n",
    " \n",
    "<img src=\"./assets/conv.png\" width=700>\n",
    " \n",
    "And let's say that you are going to convolve matrix B with Matrix A with a stride of 1, here stride means the steps that matrix B will take while convolving matrix A, let's see what it actually is to make it more clear.\n",
    " \n",
    "<img src=\"./assets/conv_opr.png\" width=700>\n",
    " \n",
    "First the matrix B goes to the first [3, 3] elements in matrix A and does a element wise multiplication with the elements of matrix B and adds all the results and the then take one step to the right which means stride is equal to one and then repeats the same operation then takes one step down and repeats the same and does this operation until it has completed the whole matrix A. the result of this operation will be a matrix with dimensions [2, 2] which is convolved with matrix B. The result of this operation is a matrix like this. \n",
    " \n",
    "<img src=\"./assets/conv_res.png\" width=250>\n",
    " \n",
    "The matrix B you just saw is formally called a filter and the one that you just saw was one of the sobel filters that are used to extract the horizontal and vertical edges in images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to see what CNNs are we need learn about two more thing from which one is a Pooling and the other is a new activation function called ReLU which we mentioned as something to try in the Neural Networks Notebook but didn't actually took up we are going to look at that in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling \n",
    " \n",
    "First let's understand Pooling by taking an example of two matrices. For this I am going to keep the matrix A same as above and will take a new matrix B.\n",
    " \n",
    "<img src=\"./assets/pool.png\" width=700>\n",
    " \n",
    "The matrix B is left blank intentionally because there is no actual second matrix in a pooling operation and the operation itself is done on the single matrix which in our case is A but to visualize pooling better we created a matrix B. This time we are going to see it with a stride of 2\n",
    " \n",
    "There are two types of Pooling:\n",
    "- Max Pooling \n",
    "- Average Pooling\n",
    " \n",
    "### Max Pooling\n",
    " \n",
    "With our example let's see what max pooling is and matrix B is going to do a max pool operation on matrix A something like this.\n",
    " \n",
    "<img src=\"./assets/pool_opr.png\" width=700>\n",
    " \n",
    "What it actually does is that it goes to the first [2, 2] elements of matrix A and selects the one which has the biggest value or the max value  out of the four elements and repeats this operation on the remaining matrix A with a stride of 2 as mentioned above. The resulting matrix is of dimensions [2, 2] and looks like this. \n",
    " \n",
    "<img src=\"./assets/max_pool_res.png\" width=250>\n",
    " \n",
    "### Average Pooling\n",
    " \n",
    "Now let's see an example of average pooling with the same two matrices and same stride but this time when matrix B does an average pool operation on matrix A it goes to the first [2, 2] elements and takes their average, which in our case for the first four elements will be $ (2 + 4 + 0 + 7) / 4 = 3.25 $ and then repeats the same operation for the whole matrix. The resulting matrix will be of dimensions [2, 2] and will look like this.\n",
    " \n",
    "<img src=\"./assets/avg_pool_res.png\" width=250>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectified Linear Unit (ReLU)\n",
    " \n",
    "The activation function ReLU is a very simple function to understand which takes in an input such as a matrix, vector or a scalar and returns the element wise operation mentioned below.\n",
    " \n",
    "<img src=\"./assets/relu.png\" width=250>\n",
    " \n",
    "So if we apply ReLU on a matrix we will get the actual elements that we supplied where the element is greater than zero and zero if the element is smaller than or equal to zero. Let's see an example. \n",
    " \n",
    "<img src=\"./assets/relu_res.png\" width=500>\n",
    " \n",
    "And if you are trying to implement this in Python it will be something like this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0]\n",
      " [0 8]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "inp = np.array([[2, 0], [-4, 8]])\n",
    "print(ReLU(inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Convolutional Layer\n",
    " \n",
    "Now armed with all the above three definitions let's see what a single convolutional layer does. \n",
    " \n",
    "So first let's say that we have a black and white image with dimensions [420 * 420 * 1] The one at the end is the channel, it's one because we have a black and white image. It would have been 3 if we had a RGB image.\n",
    " \n",
    "### Steps\n",
    " \n",
    "1. In a single convolution layer we first apply filters or basically we apply convolutions with filters for this example we will assume that our filters are of size [5 * 5 * 1] and there are 6 filters each extracts some new features, so if we convolve our image one by one with each of these filters and use a stride of one and stack the output we will get a output of dimensions [415, 415, 6] but there is one extra thing that we do with our convolution operation, each of the filter has a bias term with it which it adds after the convolutional operation. For example we have 6 filters so we have 6 bias parameters and as we have 6 filters of dimension [5, 5] we will have 150 weight parameters so a total of 156 parameters in a single convolutional layer.\n",
    " \n",
    "2. In the second step we pass that matrix that we got after the convolutional operation and addition of bias to a ReLU unit.\n",
    " \n",
    "3. And after that we generally perform a max pooling operation, you can even choose an average pooling operation but both of these operations have the same goal to reduce the size of the data / parameters. This is something you can experiment with and see which one works better with your application but mostly one or the other doesn't make much difference. \n",
    " \n",
    "With this we complete one convolutional Layer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    " \n",
    "Now we understand what a convolutional layer is. Let's finally build what we all were waiting for, A complete convolutional neural network. \n",
    " \n",
    "<img src=\"./assets/cnn.png\" width=900>\n",
    " \n",
    "> CNN Image from Cezanne Camacho's [Blog](https://cezannec.github.io/Convolutional_Neural_Networks/) \n",
    " \n",
    "In the above Image as you can see there are two Convolutional Layers, we generally don't show the type of activation function used and as you see the dimension of the image is reduced with every pooling operation and the end of the CNN you see a fully connected Layer, it's nothing but the usual Neural Network that we learned about in the previous Notebooks before the fully connected layer you see that the output is in the form of a 3 dimensional matrix to feed it to a fully connected layer we flatten the output what it means is we just arrange all of the elements of the three dimensional matrix into a row vector, and after that it is the same as the neural network that we implemented last week where we classified three flowers here we have taken an example of a car, the fully connected layers feed into a softmax unit and uses the same cross entropy loss that we used last week and can also use the same optimization algorithm but in the next notebook we are going to use some advanced optimizers that you can learn about. \n",
    " \n",
    "You can think of the convolutional layers as the feature extractor which extracts the features from images and gives them to a neural network so the neural network can classify the image. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Why Convolutional Neural Networks \n",
    " \n",
    "Now you might say why don't just use a direct Neural Network to classify the images instead of going through all of this hassel and then feeding it to the neural network. Let's look at that \n",
    " \n",
    "For our example we had an image of dimensions [420, 420] not writing the last channel as it was just one and now to feed this into our neural network we have to flatten this image into a row vector of dimension [176400, 1] and suppose we even had half the number of neurons in the next layer which which mean we will have 88200 neurons in the hidden layer and the way we find out the weight matrix dimension is that the first dimension is the neurons in the current layer and the second dimension is the neurons in the previous layer so according to that we would have had a weight matrix of size [88200, 176400] which is equal to 15558480000 parameters and i hope that you can have a rough idea of the computation cost that it will infer with just a small image like this, Now just think about a full HD image which almost all of us capture which has a dimension of [1920, 1080]\n",
    " \n",
    "The second point is that Neural networks are not generalisable with positions in images say you had a dog in the left side of a image and you labelled that as a photo of a dog and now you try to classify a image on which the dog is in the right side your neural network won't have a good time classifying it as a dog image but of the inability to generalise well with positions. here the convolutions do a great job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next\n",
    "\n",
    "In the next notebook we are going to see what transfer learning is and use that to train a Convolutional Neural Network written in tensorflow. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
